# Sistema de Agentes com RAG - 100% Gratuito

Sistema de agentes inteligentes usando **Ollama** (LLM local) + **LangChain** + **RAG** (Retrieval Augmented Generation) com embeddings TF-IDF customizados.

## ðŸŽ¯ CaracterÃ­sticas

- âœ… **100% Gratuito** - Sem APIs pagas
- âœ… **Local** - LLM Llama 3.2 (3B) via Ollama
- âœ… **RAG** - Busca semÃ¢ntica em base de conhecimento
- âœ… **Embeddings TF-IDF** - Sem dependÃªncia do PyTorch
- âœ… **Multi-agentes** - Sistema inteligente de roteamento
- âœ… **Interface Web Profissional** - Chat moderno com FastAPI + templates responsivos

## ðŸ—ï¸ Arquitetura

```
Pergunta do UsuÃ¡rio
       â†“
Sistema Multi-Agente (Router)
       â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Pergunta de RH? â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
         â”‚         â”‚
      Simâ”‚         â”‚NÃ£o
         â†“         â†“
   Agente RH   Agente Geral
       â†“
  RAG Tool
       â†“
  FAISS Index
  (TF-IDF)
       â†“
  Llama 3.2
       â†“
   Resposta
```

## ðŸ“‚ Estrutura do Projeto

```
meu_sistema_agentes/
â”œâ”€â”€ custom_embeddings.py    # Embeddings TF-IDF customizados
â”œâ”€â”€ tools.py                # Ferramenta RAG para busca
â”œâ”€â”€ ingest.py               # Script para criar Ã­ndice FAISS
â”œâ”€â”€ chat_interativo.py      # Chat interativo (principal)
â”œâ”€â”€ simple_agent.py         # Sistema multi-agente completo
â”œâ”€â”€ test_agent.py           # Testes rÃ¡pidos
â”œâ”€â”€ politica_rh.txt         # Base de conhecimento
â”œâ”€â”€ faiss_index/            # Ãndice vetorial FAISS
â””â”€â”€ README.md               # Este arquivo
```

## ðŸš€ InstalaÃ§Ã£o

### 1. Instale o Ollama

```bash
brew install ollama
brew services start ollama
ollama pull llama3.2:3b
```

### 2. Configure o ambiente Python

```bash
cd meu_sistema_agentes
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Crie o Ã­ndice FAISS

```bash
python3 ingest.py
```

### 4. Inicie a interface web

```bash
uvicorn src.web.app:app --reload
```

Acesse `http://localhost:8000` para utilizar o hub com visual profissional. Todas as requisiÃ§Ãµes continuam sendo executadas localmente, reutilizando o mesmo roteamento de agentes.

## ðŸŽ¯ Como Usar

### Chat Interativo (Recomendado)

```bash
python3 chat_interativo.py
```

FaÃ§a perguntas sobre RH e receba respostas baseadas na base de conhecimento.

### Sistema Multi-Agente Completo

```bash
python3 simple_agent.py             # modo interativo
python3 simple_agent.py --question "Qual a polÃ­tica de home office?"
python3 simple_agent.py --demo      # demonstraÃ§Ã£o automÃ¡tica
```

Use o modo interativo para fazer perguntas livremente, o parÃ¢metro `--question` para rodadas Ãºnicas e `--demo` para ver os testes prÃ©-configurados.

### Interface Web Profissional

```bash
uvicorn src.web.app:app --reload
```

- Interface moderna responsiva com histÃ³rico local de mensagens
- IndicaÃ§Ã£o visual de qual agente respondeu e se o RAG foi utilizado
- AtualizaÃ§Ã£o dinÃ¢mica no navegador via JavaScript nativo

### Teste RÃ¡pido

```bash
python3 test_agent.py
```

Testa a ferramenta RAG e o LLM separadamente.

## ðŸ“ Componentes

### Custom Embeddings (`custom_embeddings.py`)

Embeddings baseados em TF-IDF usando scikit-learn, evitando a dependÃªncia do PyTorch (incompatÃ­vel com Python 3.13).

### Ferramenta RAG (`tools.py`)

Ferramenta de busca semÃ¢ntica que:
1. Carrega o Ã­ndice FAISS
2. Busca os 3 documentos mais relevantes
3. Retorna o contexto para o LLM

### Sistema Multi-Agente (`simple_agent.py`)

**Agente RH**: Usa RAG para responder perguntas sobre polÃ­ticas de RH
**Agente Geral**: Responde perguntas gerais sem ferramentas
**Router**: Detecta automaticamente qual agente usar

## âš™ï¸ ConfiguraÃ§Ã£o

### Adicionar novos documentos

1. Adicione arquivos `.txt` na raiz do projeto
2. Modifique `ingest.py` para carregar novos arquivos
3. Execute `python3 ingest.py` para recriar o Ã­ndice

### Ajustar parÃ¢metros do LLM

Edite os arquivos Python e modifique:

```python
llm = ChatOllama(
    model="llama3.2:3b",
    temperature=0.7  # Ajuste a criatividade (0.0-1.0)
)
```

### Mudar nÃºmero de documentos retornados

Em `tools.py`:

```python
retriever = db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # Altere o nÃºmero aqui
)
```

## ðŸ› Troubleshooting

### Ollama nÃ£o estÃ¡ rodando

```bash
brew services start ollama
ollama list  # Deve mostrar llama3.2:3b
```

### Ãndice FAISS nÃ£o encontrado

```bash
python3 ingest.py
```

### Respostas lentas

- **Normal!** Modelo local 3B demora 10-15 segundos
- Primeira execuÃ§Ã£o Ã© mais lenta (carrega o modelo)
- Respostas subsequentes sÃ£o mais rÃ¡pidas

### Warning sobre PyTorch

```
None of PyTorch, TensorFlow >= 2.0, or Flax have been found.
```

**Pode ignorar!** O sistema usa TF-IDF e nÃ£o precisa de PyTorch.

## âœ… Testes e ValidaÃ§Ã£o

Para instruÃ§Ãµes completas de teste e validaÃ§Ã£o do sistema, consulte o **[Guia Completo de Testes](GUIA_TESTES.md)**.

### Testes RÃ¡pidos

```bash
# DemonstraÃ§Ã£o automÃ¡tica
python simple_agent.py --demo

# Testes automatizados
pytest -v

# Interface web
uvicorn src.web.app:app --reload
# Acesse: http://localhost:8000
```

Os testes cobrem a lÃ³gica de roteamento dos agentes e o endpoint FastAPI da interface web, garantindo a estabilidade do fluxo principal.

## ðŸ“Š ComparaÃ§Ã£o com APIs Pagas

| Aspecto | Ollama Local | OpenAI API |
|---------|-------------|------------|
| Custo | $0 | ~$0.001/resp |
| Velocidade | 10-15s | 1-2s |
| Qualidade | Boa | Excelente |
| Privacidade | 100% | Compartilhado |

## ðŸŽ“ Tecnologias

- **LangChain**: Framework para LLMs
- **Ollama**: Runtime para LLMs locais
- **FAISS**: Vector database (Facebook AI)
- **TF-IDF**: Embeddings (scikit-learn)
- **Llama 3.2**: LLM da Meta (3B parÃ¢metros)

## ðŸ“„ LicenÃ§a

MIT
